# Test Case Generator

A comprehensive test case generation system that creates high-quality test cases for software features in TestRail-compatible format. This tool analyzes existing test cases, extracts feature requirements, and generates comprehensive test coverage including positive, negative, edge cases, and accessibility scenarios.

## 📋 Current Status

This project has been enhanced with:
- ✅ **TestRail API Optimization**: Improved pagination support for handling large datasets (250+ test cases)
- ✅ **Code Reorganization**: Restructured into modular packages (`src/testrail/` and `src/jira/`)
- ✅ **Jira Integration**: Added comprehensive Jira API integration for ticket details and attachments
- ✅ **Simplified Scripts**: Streamlined `fetch_ticket_details.py` for focused functionality

## 🚀 Features

- 🤖 **AI-Powered Generation**: Uses advanced prompt engineering for intelligent test case generation
- 📊 **TestRail Integration**: Direct integration with TestRail API for seamless test case management
- 🔗 **Jira Integration**: Fetch ticket details, descriptions, and attachments from Jira tickets
- 🔍 **Duplicate Detection**: Prevents duplicate test cases using semantic similarity analysis
- ⚙️ **Configurable**: Customize test case types, counts, and priorities
- 🎯 **Comprehensive Coverage**: Generates positive, negative, edge, and accessibility test cases
- 📋 **Pattern Analysis**: Analyzes existing test cases to maintain consistency
- 🔗 **Reference ID Extraction**: Automatically extracts reference IDs from feature files
- 📎 **Attachment Management**: Download and manage ticket attachments from Jira

## 📁 Project Structure

```
Test-case-generator/
├── configs/
│   ├── config.py                   # TestRail & Jira API configuration
│   └── output_test_case_config.py  # Test case generation settings
├── feature/                        # Feature extracted from Jira ticket
│                                     or manually add files here
├── knowledgebase/                  # Test cases extracted from TestRail
├── prompts/                        # Prompts generated by Prompt orchestrator
├── src/
│   ├── testrail/                   # TestRail integration package
│   │   ├── __init__.py
│   │   ├── api.py                  # TestRail API client
│   │   └── client.py               # TestRail HTTP client
│   └── jira/                       # Jira integration package
│       ├── __init__.py
│       └── api.py                  # Jira API client
├── target/                         # Generated test case outputs
├── extract_test_cases.py           # TestRail extraction script
├── fetch_ticket_details.py         # Jira ticket details fetcher
├── generate_prompt.py              # Prompt generation orchestrator
├── save_test_cases.py              # TestRail saving utilities
├── requirements.txt                # Python dependencies
├── pyproject.toml                  # Project configuration
├── env.example                     # Environment variables template
├── LICENSE                         # MIT License file
└── README.md                       # This file
```

## 🛠️ Installation

1. **Clone the repository**:
   ```bash
   git clone <repository-url>
   cd test-case-generator
   ```

2. **Install dependencies**:
   ```bash
   pip install -r requirements.txt
   ```

3. **Set up environment variables**:
   ```bash
   cp env.example .env
   ```
   
   Edit `.env` file with your TestRail and Jira configuration:
   ```env
   # TestRail API Configuration
   TESTRAIL_URL=https://your-domain.testrail.io
   TESTRAIL_USERNAME=your-email@domain.com
   TESTRAIL_PASSWORD=your-api-key-or-password
   
   # TestRail Project Configuration
   TESTRAIL_PROJECT_ID=1
   TESTRAIL_SUITE_ID=1
   TARGET_SECTION_ID=101
   
   # Jira API Configuration
   JIRA_URL=https://your-domain.atlassian.net
   JIRA_USERNAME=your-email@domain.com
   JIRA_API_TOKEN=your-api-token
   JIRA_TICKET_ID=PROJ-123
   ```

## 🚀 Usage

### 1. Extract Existing Test Cases

First, extract existing test cases from TestRail to build your knowledge base:

```bash
python extract_test_cases.py
```

This will:
- Connect to your TestRail instance
- Extract all test cases from the configured project
- Save them to `knowledgebase/existing_test_cases.json`
- Generate a summary report

### 2. Prepare Feature Description

Create a file with the feature description in the `feature/` directory.

OR 

Use the Jira integration to fetch ticket details, descriptions, and attachments:

```bash
python fetch_ticket_details.py
```

This will:
- Connect to your Jira instance using configuration from `configs/config.py`
- Fetch details for the ticket specified in `JIRA_TICKET_ID`
- Extract ticket summary, description, and attachment information
- Save ticket details to JSON file into `feature` folder
- Download attachments to 
- Display a summary of the fetched information

### 3. Tweak the Test Case Generation Settings

Edit `configs/output_test_case_config.py` to customize:

```python
# Number of test cases to generate
DEFAULT_TEST_CASES_COUNT = 20

# Test types to generate
DEFAULT_TEST_TYPES = ["positive", "negative", "edge", "accessibility"]

# WCAG Compliance
WCAG_GUIDLINE = "WCAG 2.2 AAA"

# Priority distribution
DEFAULT_PRIORITY_DISTRIBUTION = {
    "High": 40,
    "Medium": 40, 
    "Low": 20
}

# Similarity threshold for duplicate detection
SIMILARITY_THRESHOLD = 0.85

# Fields to force/override in generated test cases (typically custom fields)
# Keys must match your TestRail field API names. Values are written as-is to
# each generated test case during normalization, overriding inferred values.
OVERRIDE_FIELDS = {
    "type_id": 22,
    "custom_automation_type": 6,
    "custom_platforms": 2,
    "custom_squad": 2,
    "custom_levels": 2,
    "custom_operatingsystem": "",
    "custom_testtype": [1],
}
```

### OVERRIDE_FIELDS

`OVERRIDE_FIELDS` lets you explicitly set or normalize specific TestRail fields on every generated test case. This is useful to enforce your team's defaults or meet template requirements regardless of what is inferred from existing patterns.

- **When it's applied**: After the prompt is generated and test cases are produced, a normalization pass applies these overrides to each test case object before saving to `target/generated_test_cases.json` or uploading to TestRail.
- **Why is it useful?**  
  Using `OVERRIDE_FIELDS` ensures that every generated test case consistently matches your team's standards or project requirements, regardless of what the AI or existing patterns might suggest. This is especially helpful for enforcing required field values, meeting template constraints, or clearing out unwanted data. By explicitly setting or clearing fields, you avoid manual edits and reduce the risk of errors when importing cases into TestRail.

Examples:

```python
# Enforce team defaults across all generated cases
OVERRIDE_FIELDS = {
    "type_id": 22,                  # Force case type
    "priority_id": 2,               # Medium priority
    "custom_automation_type": 6,    # Team-specific enum value
    "custom_platforms": 2,          # Platform enum
    "custom_squad": 2,              # Squad enum
    "custom_levels": 2,             # Testing level enum
    "custom_operatingsystem": "",  # Clear text field
    "custom_testtype": [1],         # Multi-select (e.g., Functional)
}

# Clear or disable fields deliberately
OVERRIDE_FIELDS = {
    "refs": "",                    # Remove references
    "custom_testtype": [],          # No test type tags
}

# Disable overrides entirely
OVERRIDE_FIELDS = {}
```

Notes:
- These overrides take precedence over values inferred from existing cases or the AI prompt output.
- Only fields present in your TestRail project/template will be accepted by the API. Ensure IDs map to valid options in your instance.
- For `custom_steps_separated`, prefer allowing the generator to build steps, but you can still normalize shape if needed (array of `{ content, expected }`).

### 4. Generate Prompt for Test Case Creation

Run the prompt generation orchestrator:

```bash
python generate_prompt.py
```

This will:
- Load existing test cases from `knowledgebase/`
- Analyze patterns and structure
- Read feature files from `feature/`
- Extract requirements and reference IDs
- Read configuration from `configs/output_test_case_config.py`
- Generate a comprehensive prompt in `prompts/test_case_generator.md`

### 5. Use the Generated Prompt

The generated prompt in `prompts/test_case_generator.md` contains:
- **Pattern Analysis**: Understanding of existing test case structure
- **Feature Requirements**: Extracted requirements from feature files
- **Configuration**: Test case count, types, and priority distribution
- **Instructions**: Step-by-step guidance for generating test cases
- **Quality Checks**: Validation criteria for generated test cases

After you run `python generate_prompt.py`, a complete, ready-to-use prompt is written to `prompts/test_case_generator.md`.

**Steps to use it in Cursor:**

- Open `prompts/test_case_generator.md` and copy its entire content.
- In Cursor, start a new chat with the AI.
- Paste the content as your initial prompt.

OR

- Simply write "Use the file `@test_case_generator.md` as your prompt and follow the instructions in it"

- The prompt uses `@` file references (e.g., `@knowledgebase/existing_test_cases.json`, `@target/generated_test_cases.json`, `@feature/*.*`) so Cursor can load large files efficiently.
- When the AI finishes generating the test cases, click "Keep All" to save the JSON array to `target/generated_test_cases.json`.

Tip: If you change config in `configs/output_test_case_config.py` or update features/knowledge base, rerun `python generate_prompt.py` and paste the refreshed prompt again.

**Using the Prompt in GitHub Copilot Chat (VS Code)**

The generated prompt already includes dual path forms so it works in Copilot too. Steps:

- Open `prompts/test_case_generator.md` and copy the prompt.
- Paste it into Copilot Chat.

OR

- Simply attach the `test_case_generator.md` in the Co-pilot chat and write the prompt "use the content of the file as your prompt and follow the instructions in it"
- Copilot can use either path form in the prompt:
   - Knowledge base: `@knowledgebase/existing_test_cases.json` or `knowledgebase/existing_test_cases.json`
   - Generated cases: `@target/generated_test_cases.json` or `target/generated_test_cases.json`
   - Feature docs: `@feature/*.*` or `feature/*.*`
4. Ask Copilot to read those files and generate the JSON array.
5. Save to `target/generated_test_cases.json`. If needed, ask Copilot to return the JSON in one fenced block.

### 6. Save Test Cases to TestRail

Use the save utility to upload generated test cases to TestRail:

```bash
python save_test_cases.py
```

This will:
- Load test cases from `target/generated_test_cases.json`
- Format them for TestRail
- Upload to your TestRail project

## 📊 Output Formats

### JSON Format (Flexible, TestRail-Compatible)

Generated test cases follow the template and fields detected from your existing and recently generated cases. The schema is derived dynamically (including custom fields) and only includes fields that are not auto-generated by TestRail. This means it works with any TestRail test case template as long as the fields appear in your data.

Example (your schema may differ):

```json
{
  "title": "User Login with Valid Credentials",
  "template_id": 2,
  "type_id": 16,
  "priority_id": 2,
  "refs": "LOGIN-001",
  "estimate": "5min",
  "custom_preconds": "User has a registered account",
  "custom_steps_separated": [
    {
      "content": "Navigate to login page",
      "expected": "Login form is displayed"
    }
  ],
  "labels": ["login", "authentication"],
  "custom_automation_type": 0,
  "custom_platforms": 0,
  "custom_testtype": []
}
```

Notes:
- Numeric fields use numeric placeholders (e.g., `0`) instead of empty strings.
- `custom_steps_separated` is normalized to an array of step objects with `content` and `expected`.
- JSON content is preserved for fields like `custom_preconds`, step `content`, and `expected` when present.

## 🎯 Test Case Types

The system generates comprehensive test coverage:

### Positive Tests
- Valid user inputs and workflows
- Successful system responses
- Happy path scenarios
- Expected functionality verification

### Negative Tests
- Invalid inputs and error conditions
- Exception handling verification
- Security vulnerability testing
- Graceful failure mode validation

### Edge Tests
- Boundary condition testing
- Performance limit validation
- Data limit verification
- Concurrent operation testing

### Accessibility Tests (Optional)
- WCAG compliance verification
- Screen reader compatibility
- Keyboard navigation testing
- Color contrast validation
- Focus management testing

## 🔍 Duplicate Detection

The system includes intelligent duplicate detection:

- **Semantic Analysis**: Compares test case content and intent
- **Similarity Scoring**: Uses configurable threshold (default: 0.85)
- **Reference ID Tracking**: Extracts and uses reference IDs from feature files
- **Quality Filtering**: Only saves potential duplicates that meet similarity criteria

## 🔗 Jira Integration

The system includes comprehensive Jira integration for ticket management:

### Ticket Details Fetching
- **Summary Extraction**: Retrieves ticket summaries and descriptions
- **ADF Support**: Handles Atlassian Document Format for rich text descriptions
- **Attachment Management**: Downloads and organizes ticket attachments
- **Metadata Extraction**: Captures ticket metadata (author, creation date, etc.)

## 🔧 Troubleshooting

### Common Issues

1. **TestRail Connection Failed**
   - Verify your TestRail URL, username, and password in `.env`
   - Check if your TestRail instance allows API access
   - Ensure your account has appropriate permissions

2. **No Test Cases Generated**
   - Ensure `knowledgebase/existing_test_cases.json` exists
   - Check that feature files are in the `feature/` directory
   - Verify configuration in `configs/output_test_case_config.py`

3. **Prompt Generation Errors**
   - Check that all required directories exist
   - Verify file permissions and access
   - Review console output for specific error messages

4. **Duplicate Detection Issues**
   - Adjust `SIMILARITY_THRESHOLD` in configuration
   - Review similarity analysis results
   - Check reference ID extraction from feature files

5. **Jira Connection Issues**
   - Verify your Jira URL, username, and API token in `.env`
   - Ensure your Jira instance allows API access
   - Check that the ticket ID exists and is accessible
   - Verify your account has appropriate permissions for the ticket

### Debug Mode

Enable debug output by setting the `DEBUG` environment variable:

```bash
export DEBUG=1
python generate_prompt.py
```

## 🤝 Contributing

1. Fork the repository
2. Create a feature branch
3. Make your changes
4. Submit a pull request

## 📄 License

This project is licensed under the MIT License - see the [LICENSE](LICENSE) file for details.

### License Summary
The MIT License is a permissive open-source license that allows you to:
- ✅ Use the software for any purpose
- ✅ Modify the software
- ✅ Distribute the software
- ✅ Use it commercially
- ✅ Use it privately
- ✅ Sublicense it

The only requirement is that the original license and copyright notice must be included in all copies or substantial portions of the software.

### Copyright
Copyright (c) 2025 Test Case Generator

For more information about the MIT License, visit: https://opensource.org/licenses/MIT

## 🆘 Support

For support and questions:
- Create an issue in the repository
- Check the troubleshooting section
- Review the configuration documentation
- Run the test suite to verify functionality
