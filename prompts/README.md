# Test Case Generation Prompts

## 🎯 Overview

This folder contains the comprehensive prompt system for generating test cases using AI. The system is designed to analyze existing test cases, understand project patterns, read feature requirements, and generate high-quality test cases that are compatible with TestRail.

## 📁 Files

### `test_case_generator.md`
The main prompt file that is dynamically generated by `generate_prompt.py` and provides comprehensive instructions for AI to:
- Analyze existing test cases from the knowledge base
- Read and understand feature requirements
- Extract reference IDs from feature files
- Generate test cases following established patterns
- Perform duplicate detection with similarity analysis
- Save output to the target folder in TestRail-compatible format

### `README.md` (this file)
Documentation for using the prompt system

## 🚀 How to Use

### Step 1: Run the Orchestrator
First, run the orchestrator script to analyze your project:

```bash
python generate_prompt.py
```

This will:
- Read existing test cases from `knowledgebase/existing_test_cases.json`
- Analyze patterns and structure (test types, priorities, naming conventions)
- Read feature files from `feature/` directory
- Extract requirements and reference IDs
- Read configuration from `configs/output_test_case_config.py`
- Generate a comprehensive prompt in `prompts/test_case_generator.md`

### Step 2: Use the Generated Prompt
1. Open the `prompts/test_case_generator.md` file
2. Copy the entire content
3. Use it as a prompt for your AI tool (Cursor, ChatGPT, etc.)
4. The AI will follow the workflow to generate test cases

### Step 3: Execute the Workflow
The AI will:
1. **Analyze existing test cases** from `knowledgebase/existing_test_cases.json`
2. **Read feature files** from the `feature/` folder
3. **Extract reference IDs** from feature files for the `refs` field
4. **Understand the project format** and patterns
5. **Generate comprehensive test cases** following the established structure
6. **Perform duplicate detection** using similarity analysis
7. **Save to target folder** with proper naming and formatting

## 📊 Expected Output

The generated test cases will be saved in the `target/` folder with:
- **JSON format** following the exact TestRail API structure
- **Comprehensive coverage** of all requirements
- **Proper categorization** by type and priority
- **Reference IDs** extracted from feature files
- **Duplicate analysis** with similarity scores
- **Summary files** with breakdown statistics

## 🎯 Test Case Types

The system generates comprehensive test coverage:

### Positive Tests
- Valid user inputs and workflows
- Successful system responses
- Happy path scenarios
- Expected functionality verification

### Negative Tests
- Invalid inputs and error conditions
- Exception handling verification
- Security vulnerability testing
- Graceful failure mode validation

### Edge Tests
- Boundary condition testing
- Performance limit validation
- Data limit verification
- Concurrent operation testing

### Accessibility Tests (Optional)
- WCAG compliance verification
- Screen reader compatibility
- Keyboard navigation testing
- Color contrast validation
- Focus management testing

## ⚙️ Configuration

The system uses configuration from `configs/output_test_case_config.py`:

```python
# Number of test cases to generate
DEFAULT_TEST_CASES_COUNT = 20

# Test types to generate
DEFAULT_TEST_TYPES = ["positive", "negative", "edge"]

# Priority distribution
DEFAULT_PRIORITY_DISTRIBUTION = {
    "High": 40,
    "Medium": 40,
    "Low": 20
}

# Similarity threshold for duplicate detection
SIMILARITY_THRESHOLD = 0.85
```

## 📋 Test Case Structure

Each generated test case follows the TestRail API format:

```json
{
  "title": "Descriptive test case title",
  "template_id": 2,
  "type_id": 16,
  "priority_id": 2,
  "refs": "REF-001",
  "estimate": "5min",
  "custom_preconds": "Prerequisites and setup requirements",
  "custom_steps_separated": [
    {
      "content": "Step description",
      "expected": "Expected result"
    }
  ],
  "labels": ["tag1", "tag2"]
}
```

## 🔍 Duplicate Detection

The prompt includes intelligent duplicate detection:

- **Semantic Analysis**: Compares test case content and intent
- **Similarity Scoring**: Uses configurable threshold (default: 0.85)
- **Reference ID Tracking**: Extracts and uses reference IDs from feature files
- **Quality Filtering**: Only saves potential duplicates that meet similarity criteria

## 🔍 Quality Assurance

The prompt includes a comprehensive quality checklist:

- [ ] All functional requirements are covered
- [ ] Security requirements are addressed
- [ ] Accessibility requirements are included (if applicable)
- [ ] Performance requirements are considered
- [ ] Test cases follow the TestRail API format exactly
- [ ] Steps are clear and actionable
- [ ] Expected results are specific and measurable
- [ ] Reference IDs are extracted from feature files
- [ ] Priority levels are correctly assigned
- [ ] Test types are properly categorized
- [ ] Duplicate detection is performed
- [ ] Similarity scores are calculated

## 🎯 Success Criteria

- Test cases are comprehensive and cover all requirements
- Format matches the TestRail API exactly
- All test types are represented according to configuration
- Priority distribution follows the specified percentages
- Reference IDs are extracted and used correctly
- Test cases are actionable and clear
- Output is saved to the correct location
- Duplicate detection is performed with similarity analysis
- Quality standards are maintained

## 📝 Example Usage

1. **Prepare your project**:
   - Ensure `knowledgebase/existing_test_cases.json` exists
   - Add feature files to the `feature/` folder
   - Configure settings in `configs/output_test_case_config.py`

2. **Run the orchestrator**:
   ```bash
   python generate_prompt.py
   ```

3. **Use with AI**:
   - Open `prompts/test_case_generator.md`
   - Copy the content as a prompt for your AI tool
   - Execute the workflow

4. **Review output**:
   - Check generated test cases in `target/` folder
   - Review duplicate analysis results
   - Validate against requirements

## 🚀 Benefits

- **Consistent Format**: Follows TestRail API format exactly
- **Comprehensive Coverage**: Addresses all requirement types
- **Pattern Consistency**: Maintains established test case patterns
- **Duplicate Prevention**: Intelligent similarity analysis
- **Reference Tracking**: Automatic extraction of reference IDs
- **Quality Assurance**: Built-in quality checks and validation
- **Scalable**: Works with any feature or project
- **Configurable**: Easy to adjust settings and requirements
- **Documented**: Clear instructions and examples

## 🔧 Advanced Features

### Pattern Analysis
The system analyzes existing test cases to understand:
- Test type distribution and patterns
- Priority level assignments
- Naming conventions and styles
- Step structure and formatting
- Custom field usage
- Common preconditions and requirements

### Reference ID Extraction
Automatically extracts reference IDs from feature files:
- Looks for patterns like "REF-001", "TC-123", etc.
- Uses extracted IDs in the `refs` field
- Maintains traceability between features and test cases

### Similarity Analysis
Performs semantic analysis to detect duplicates:
- Compares test case content and intent
- Calculates similarity scores
- Filters results based on configurable threshold
- Provides detailed analysis in output files

---

**Ready to generate test cases? Follow the workflow above and let AI create comprehensive, high-quality test cases for your project!** 
